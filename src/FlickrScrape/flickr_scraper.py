import sys
sys.path.insert(0,'./src')
sys.path.insert(0,'./src/FlickrScrape')

# Generated by Glenn Jocher (glenn.jocher@ultralytics.com) for https://github.com/ultralytics
import argparse
import os
import time
import cv2

import Constants
from Utils import get_flickr_creds, exit_after
from flickrapi import FlickrAPI
from flickr_utils.general import download_uri

SCRAPE_COUNT = 4000
COUNTRIES_SCRAPE_COUNT = 500
COUNTRIES = []
MAX_QUALITY = 2500
PER_PAGE = 500 # 1-500

IMAGE_DOWNLOAD_TIMEOUT = 20


# Return true if reduced
def reduce_resolution(img_pth):
    try:
        img = cv2.imread(img_pth)
        if img is None:
            return None
    except:
        return None
    
    img_rescaled = False
    
    if max(img.shape[0], img.shape[1])  > MAX_QUALITY:
        scale = MAX_QUALITY / max(img.shape[0], img.shape[1])
        img = cv2.resize(img, (int(scale * img.shape[1]), int(scale * img.shape[0])))
        img_rescaled = True
        
    img_pth_jpg = ".".join(img_pth.split(".")[:-1]) + ".jpg"
    # Resaves image into a jpg or resive if it was resized
    if img_rescaled or img_pth_jpg != img_pth:
        cv2.imwrite(img_pth_jpg, img)
        if img_pth != img_pth_jpg:
            os.remove(img_pth)

    return img_rescaled

@exit_after(IMAGE_DOWNLOAD_TIMEOUT)
def download_image(url, raw_dir):
    img_pth = download_uri(url, raw_dir)
    rescaled = reduce_resolution(img_pth)
    # remove images that weren't loadable
    if rescaled is None:
        os.remove(img_pth)
        img_pth = None
        
    return img_pth
    
def flickr_scrape(search="city portrait", n=10, raw_dir=os.path.join(Constants.RAW_BODY_IMAGES_DIR, "test"), clean_queue=None):
    actual_search = search
        
    # Gets the flickr creds
    key, secret = get_flickr_creds()
    
    t = time.time()
    flickr = FlickrAPI(key, secret)
    license = ()  # https://www.flickr.com/services/api/explore/?method=flickr.photos.licenses.getInfo
    
    if not os.path.exists(raw_dir):
        os.makedirs(raw_dir)  
    
    photos = flickr.walk(
        text=actual_search,  # http://www.flickr.com/services/api/flickr.photos.search.html
        extras="url_o",
        per_page=PER_PAGE,   # 1-500
        license=license,
        sort="relevance",
        media="photos",
    )
    
    amount = 0  
    start_time = time.time()
    # print(photos.keys)
    for photo in photos:
        if amount > n:
            break
 
        try:
            # construct url https://www.flickr.com/services/api/misc.urls.html
            url = photo.get("url_o")  # original size
            if url is None:
                url = f"https://farm{photo.get('farm')}.staticflickr.com/{photo.get('server')}/{photo.get('id')}_{photo.get('secret')}_b.jpg"

            try:
                img_pth = download_image(url, raw_dir)        
            except Exception as err:
                img_pth = None
                print("Error when attempting to download image:", err)
                
            if clean_queue is not None and img_pth is not None:
                clean_queue.put(img_pth)
                print(f"Saved an {search} image {amount}")
                
        except:
            print("%g/%g error..." % (amount, n))
        
        remaining_time = float(amount + 1) - (time.time()- start_time)
        if remaining_time > 0:
            time.sleep(remaining_time)
        
        amount += 1        
            
            
    print("Done. (%.1fs)" % (time.time() - t) + ("\nAll images saved to %s" % raw_dir))


def is_country(query):
    country = " ".join(query.split(" ")[:-1])
    return country in COUNTRIES


def body_scrape(query_list : list[str], clean_queue=None, lock=None): 
    # Creates file if it doesnt exist
    if not os.path.isfile(Constants.FINIHSED_BODY_RAW_TXT):
        file = open(Constants.FINIHSED_BODY_RAW_TXT, 'w')
        file.close()
        
        
    # Instanties then countries constant
    with open(Constants.COUNTRIES_FILES,'r') as file:
        for x in file.readlines():
            line = x.strip()
            COUNTRIES.append(line)
        
    for query in query_list:
        n = SCRAPE_COUNT if not is_country(query) else COUNTRIES_SCRAPE_COUNT
        
        print(f"Scraping {n} imagse from {query}")
            
        output_dir = os.path.join(Constants.RAW_BODY_IMAGES_DIR, query)
        flickr_scrape(search=query, n=n, clean_queue=clean_queue, raw_dir=output_dir)
        
        if lock is None:
            continue 
        # Records that it finished scraping
        with lock:
            with open(Constants.FINIHSED_BODY_RAW_TXT, 'a') as raw_finished_file:
                raw_finished_file.write(f'\n{query}')
    
     
if __name__ == "__main__":

    flickr_scrape(search="city portrait extra")