import sys

import psutil
sys.path.insert(0,'./src')
sys.path.insert(0,'./src/FlickrScrape')

# Generated by Glenn Jocher (glenn.jocher@ultralytics.com) for https://github.com/ultralytics
import argparse
import os
import time
import cv2

import Constants
from Utils import get_flickr_creds, exit_after
from flickrapi import FlickrAPI

from flickr_utils.general import download_uri

MAX_QUALITY = 5000
PER_PAGE = 500  

IMAGE_DOWNLOAD_TIMEOUT = 20

TIME_PER_IMAGE = 0.1


# Return true if reduced
def reduce_resolution(img_pth):
    try:
        img = cv2.imread(img_pth)
        if img is None:
            return None
    except:
        return None
    
    img_rescaled = False
    
    if max(img.shape[0], img.shape[1])  > MAX_QUALITY:
        scale = MAX_QUALITY / max(img.shape[0], img.shape[1])
        img = cv2.resize(img, (int(scale * img.shape[1]), int(scale * img.shape[0])))
        img_rescaled = True
        
    img_pth_jpg = ".".join(img_pth.split(".")[:-1]) + ".jpg"
    # Resaves image into a jpg or resive if it was resized
    if img_rescaled or img_pth_jpg != img_pth:
        cv2.imwrite(img_pth_jpg, img)
        if img_pth != img_pth_jpg:
            os.remove(img_pth)

    return img_rescaled

@exit_after(IMAGE_DOWNLOAD_TIMEOUT)
def download_image(url, raw_dir):
    img_pth = download_uri(url, raw_dir)
    rescaled = reduce_resolution(img_pth)
    # remove images that weren't loadable
    if rescaled is None:
        os.remove(img_pth)
        img_pth = None
        
    return img_pth
    
def flickr_scrape(group_name, group_id, raw_dir=os.path.join(Constants.RAW_BODY_IMAGES_DIR, "test"), clean_queue=None):
    key, secret = get_flickr_creds()
    
    t = time.time()
    flickr = FlickrAPI(key, secret)
        
    if not os.path.exists(raw_dir):
        os.makedirs(raw_dir)  
    
    photos = flickr.data_walker(
            flickr.groups.pools.getPhotos,
            per_page=PER_PAGE,
            group_id=group_id,
            extras="url_o",
            )
    
    amount = 0  
    start_time = time.time()
    # print(photos.keys)
    for photo in photos:
        try:
            # construct url https://www.flickr.com/services/api/misc.urls.html
            url = photo.get("url_o")  # original size
            if url is None:
                url = f"https://farm{photo.get('farm')}.staticflickr.com/{photo.get('server')}/{photo.get('id')}_{photo.get('secret')}_b.jpg"

            attempts = 0
            while attempts < 3:
                try:
                    img_pth = download_image(url, raw_dir)        
                    break
                except Exception as err:
                    img_pth = None
                    print(f"Error when attempting to download image (Attempt: {attempts}):", err)
                    
                attempts += 1
                
            if clean_queue is not None and img_pth is not None:
                clean_queue.put(img_pth)
                print(f"Saved an {group_name} image {amount}")
                
        except:
            print("%g/%g error..." % (amount))
        
        remaining_time = float(TIME_PER_IMAGE  * (amount + 1)) - (time.time()- start_time)
        if remaining_time > 0:
            time.sleep(remaining_time)
        
        amount += 1        
            
            
    print("Done. (%.1fs)" % (time.time() - t) + ("\nAll images saved to %s" % raw_dir))


def body_scrape(group_list : list[tuple[str, str]], clean_queue=None, lock=None): 
    p = psutil.Process(os.getpid())
    p.nice(psutil.HIGH_PRIORITY_CLASS)  # set
    
    # Creates file if it doesnt exist
    if not os.path.isfile(Constants.FINIHSED_BODY_RAW_TXT):
        file = open(Constants.FINIHSED_BODY_RAW_TXT, 'w')
        file.close()
        
        
    for group in group_list:
        group_name, group_id = group[0], group[2]
        print(f"Scraping images from {group_name}")
                
        output_dir = os.path.join(Constants.RAW_BODY_IMAGES_DIR, group_name)
        flickr_scrape(group_name=group_name, group_id=group_id, clean_queue=clean_queue, raw_dir=output_dir)
        
        if lock is None:
            continue 
        # Records that it finished scraping
        with lock:
            with open(Constants.FINIHSED_BODY_RAW_TXT, 'a') as raw_finished_file:
                raw_finished_file.write(f'\n{group_name}')
    
  